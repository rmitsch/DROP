record_name;target_label;original_abstract;year;url;processed_title;processed_abstract
LiveGantt: Interactively Visualizing a Large Manufacturing Schedule;['simplification'];In this paper, we introduce LiveGantt as a novel interactive schedule visualization tool that helps users explore highly-concurrent large schedules from various perspectives. Although a Gantt chart is the most common approach to illustrate schedules, currently available Gantt chart visualization tools suffer from limited scalability and lack of interactions. LiveGantt is built with newly designed algorithms and interactions to improve conventional charts with better scalability, explorability, and reschedulability. It employs resource reordering and task aggregation to display the schedules in a scalable way. LiveGantt provides four coordinated views and filtering techniques to help users explore and interact with the schedules in more flexible ways. In addition, LiveGantt is equipped with an efficient rescheduler to allow users to instantaneously modify their schedules based on their scheduling experience in the fields. To assess the usefulness of the application of LiveGantt, we conducted a case study on manufacturing schedule data with four industrial engineering researchers. Participants not only grasped an overview of a schedule but also explored the schedule from multiple perspectives to make enhancements.;2014;http://dx.doi.org/10.1109/TVCG.2014.2346454;livegantt interactively large manufacturing schedule;livegantt interactive schedule tool helps users explore highlyconcurrent large schedules various perspectives although gantt chart illustrate schedules currently available gantt chart tools suffer limited scalability lack interactions livegantt built designed algorithms interactions improve conventional charts scalability explorability reschedulability employs resource reordering task aggregation display schedules scalable way livegantt provides four coordinated views filtering techniques help users explore interact schedules flexible ways addition livegantt equipped efficient rescheduler allow users instantaneously modify schedules based scheduling experience fields assess usefulness application livegantt conducted case manufacturing schedule four industrial engineering researchers participants grasped overview schedule explored schedule multiple perspectives make enhancements
Decomposition and Simplification of Multivariate Data using Pareto Sets;['simplification'];"Topological and structural analysis of multivariate data is aimed at improving the understanding and usage of such data through identification of intrinsic features and structural relationships among multiple variables. We present two novel methods for simplifying so-called Pareto sets that describe such structural relationships. Such simplification is a precondition for meaningful visualization of structurally rich or noisy data. As a framework for simplification operations, we introduce a decomposition of the data domain into regions of equivalent structural behavior and the reachability graph that describes global connectivity of Pareto extrema. Simplification is then performed as a sequence of edge collapses in this graph; to determine a suitable sequence of such operations, we describe and utilize a comparison measure that reflects the changes to the data that each operation represents. We demonstrate and evaluate our methods on synthetic and real-world examples.";2014;http://dx.doi.org/10.1109/TVCG.2014.2346447;decomposition simplification multivariate pareto sets;topological structural analysis multivariate aimed improving understanding usage identification intrinsic features structural relationships among multiple variables two methods simplifying socalled pareto sets structural relationships simplification precondition meaningful structurally rich noisy framework simplification operations decomposition domain regions equivalent structural behavior reachability graph describes global connectivity pareto extrema simplification performed sequence edge collapses graph determine suitable sequence operations utilize comparison measure reflects changes operation represents demonstrate evaluate methods synthetic realworld examples
Temporal Event Sequence Simplification;['simplification'];Electronic Health Records (EHRs) have emerged as a cost-effective data source for conducting medical research. The difficulty in using EHRs for research purposes, however, is that both patient selection and record analysis must be conducted across very large, and typically very noisy datasets. Our previous work introduced EventFlow, a visualization tool that transforms an entire dataset of temporal event records into an aggregated display, allowing researchers to analyze population-level patterns and trends. As datasets become larger and more varied, however, it becomes increasingly difficult to provide a succinct, summarizing display. This paper presents a series of user-driven data simplifications that allow researchers to pare event records down to their core elements. Furthermore, we present a novel metric for measuring visual complexity, and a language for codifying disjoint strategies into an overarching simplification framework. These simplifications were used by real-world researchers to gain new and valuable insights from initially overwhelming datasets.;2013;http://dx.doi.org/10.1109/TVCG.2013.200;temporal event sequence simplification;electronic health records ehrs emerged costeffective source conducting medical research difficulty ehrs research purposes however patient selection record analysis must conducted across large typically noisy datasets introduced eventflow tool transforms entire dataset temporal event records aggregated display allowing researchers analyze populationlevel patterns trends datasets become larger varied however increasingly difficult provide succinct summarizing display presents series userdriven simplifications allow researchers pare event records core elements furthermore metric measuring complexity language codifying disjoint strategies overarching simplification framework simplifications realworld researchers gain valuable insights initially overwhelming datasets
Efficient Computation of Morse-Smale Complexes for Three-dimensional Scalar Functions;['multi-resolution', 'simplification', 'feature detection'];The Morse-Smale complex is an efficient representation of the gradient behavior of a scalar function, and critical points paired by the complex identify topological features and their importance. We present an algorithm that constructs the Morse-Smale complex in a series of sweeps through the data, identifying various components of the complex in a consistent manner. All components of the complex, both geometric and topological, are computed, providing a complete decomposition of the domain. Efficiency is maintained by representing the geometry of the complex in terms of point sets.;2007;http://dx.doi.org/10.1109/TVCG.2007.70552;efficient computation morsesmale complexes threedimensional scalar functions;morsesmale complex efficient representation gradient behavior scalar function critical points paired complex identify topological features importance algorithm constructs morsesmale complex series sweeps identifying various components complex consistent manner components complex geometric topological computed providing complete decomposition domain efficiency maintained representing geometry complex terms point sets
Topology-based simplification for feature extraction from 3D scalar fields;['multi-resolution', 'simplification', 'feature detection'];In this paper, we present a topological approach for simplifying continuous functions defined on volumetric domains. We introduce two atomic operations that remove pairs of critical points of the function and design a combinatorial algorithm that simplifies the Morse-Smale complex by repeated application of these operations. The Morse-Smale complex is a topological data structure that provides a compact representation of gradient flow between critical points of a function. Critical points paired by the Morse-Smale complex identify topological features and their importance. The simplification procedure leaves important critical points untouched, and is therefore useful for extracting desirable features. We also present a visualization of the simplified topology.;2005;http://dx.doi.org/10.1109/VISUAL.2005.1532839;topologybased simplification feature extraction scalar fields;topological simplifying continuous functions defined volumetric domains two atomic operations remove pairs critical points function design combinatorial algorithm simplifies morsesmale complex repeated application operations morsesmale complex topological structure provides compact representation gradient flow critical points function critical points paired morsesmale complex identify topological features importance simplification procedure leaves important critical points untouched therefore useful extracting desirable features simplified topology
Centroidal Voronoi tessellation based algorithms for vector fields visualization and segmentation;['flowvisualization', 'simplification'];A new method for the simplification and the visualization of vector fields is presented based on the notion of centroidal Voronoi tessellations (CVT's). A CVT is a special Voronoi tessellation for which the generators of the Voronoi regions in the tessellation are also the centers of mass (or means) with respect to a prescribed density. A distance function in both the spatial and vector spaces is introduced to measure the similarity of the spatially distributed vector fields. Based on such a distance, vector fields are naturally clustered and their simplified representations are obtained. Our method combines simple geometric intuitions with the rigorously established optimality properties of the CVTs. It is simple to describe, easy to understand and implement. Numerical examples are also provided to illustrate the effectiveness and competitiveness of the CVT-based vector simplification and visualization methodology.;2004;http://dx.doi.org/10.1109/VISUAL.2004.13;centroidal voronoi tessellation based algorithms vector fields segmentation;method simplification vector fields presented based notion centroidal voronoi tessellations cvts cvt special voronoi tessellation generators voronoi regions tessellation centers mass means respect prescribed density distance function spatial vector spaces introduced measure similarity spatially distributed vector fields based distance vector fields naturally clustered simplified representations obtained method combines simple geometric intuitions rigorously established optimality properties cvts simple easy implement numerical examples illustrate effectiveness competitiveness cvtbased vector simplification methodology
Appearance-preserving view-dependent visualization;['level-of-detail', 'mesh simplification', 'multi-resolution model'];In this paper a new quadric-based view- dependent simplification scheme is presented. The scheme provides a method to connect mesh simplification controlled by a quadric error metric with a level- of-detail hierarchy that is accessed continuously and efficiently based on current view parameters. A variety of methods for determining the screen-space metric for the view calculation are implemented and evaluated, including an appearance-preserving method that has both geometry- and texture-preserving aspects. Results are presented and compared for a variety of models.;2003;http://dx.doi.org/10.1109/VISUAL.2003.1250409;appearancepreserving viewdependent;quadricbased view dependent simplification scheme presented scheme provides method connect mesh simplification controlled quadric error metric level ofdetail hierarchy accessed continuously efficiently based current view parameters variety methods determining screenspace metric view calculation implemented evaluated including appearancepreserving method geometry texturepreserving aspects results presented compared variety models
Visibility based methods and assessment for detail-recovery;['simplification'];"In this paper we propose a new method for the creation of normal maps for recovering the detail on simplified meshes and a set of objective techniques to metrically evaluate the quality of different recovering techniques. The proposed techniques, that automatically produces a normal-map texture for a simple 3D model that \""imitates\"" the high frequency detail originally present in a second, much higher resolution one, is based on the computation of per-texel visibility and self-occlusion information. This information is used to define a point-to-point correspondence between simplified and hires meshes. Moreover, we introduce a number of criteria for measuring the quality (visual or otherwise) of a given mapping method, and provide efficient algorithms to implement them. Lastly, we apply them to rate different mapping methods, including the widely used ones and the new one proposed here.";2003;http://dx.doi.org/10.1109/VISUAL.2003.1250407;visibility based methods assessment detailrecovery;propose method creation normal maps recovering detail simplified meshes set objective techniques metrically evaluate quality different recovering techniques proposed techniques automatically produces normalmap texture simple model imitates high frequency detail originally second higher resolution one based computation pertexel visibility selfocclusion information information define pointtopoint correspondence simplified hires meshes moreover number criteria measuring quality otherwise mapping method provide efficient algorithms implement lastly rate different mapping methods including widely ones one proposed
A multi-resolution data structure for two-dimensional Morse-Smale functions;['simplification'];We combine topological and geometric methods to construct a multi-resolution data structure for functions over two-dimensional domains. Starting with the Morse- Smale complex, we construct a topological hierarchy by progressively canceling critical points in pairs. Concurrently, we create a geometric hierarchy by adapting the geometry to the changes in topology. The data structure supports mesh traversal operations similarly to traditional multi-resolution representations.;2003;http://dx.doi.org/10.1109/VISUAL.2003.1250365;multiresolution structure twodimensional morsesmale functions;combine topological geometric methods construct multiresolution structure functions twodimensional domains starting morse smale complex construct topological hierarchy progressively canceling critical points pairs concurrently create geometric hierarchy adapting geometry changes topology structure supports mesh traversal operations similarly traditional multiresolution representations
Immersive volume visualization of seismic simulations: A case study of techniques invented and lessons learned;['level-of-detail', 'multi-resolution', 'mesh simplification'];"This paper is a documentation of techniques invented, results obtained and lessons learned while creating visualization algorithms to render outputs of large-scale seismic simulations. The objective is the development of techniques for a collaborative simulation and visualization shared between structural engineers, seismologists, and computer scientists. The computer graphics research community has been witnessing a large number of exemplary publications addressing the challenges faced while trying to visualize both large-scale surface and volumetric datasets lately. From a visualization perspective, issues like data preprocessing (simplification, sampling, filtering, etc.); rendering algorithms (surface and volume), and interaction paradigms (large-scale, highly interactive, highly immersive, etc.) have been areas of study. In this light, we outline and describe the milestones achieved in a large-scale simulation and visualization project, which opened the scope for combining existing techniques with new methods, especially in those cases where no existing methods were suitable. We elucidate the data simplification and reorganization schemes that we used, and discuss the problems we encountered and the solutions we found. We describe both desktop (high-end local as well as remote) interfaces and immersive visualization systems that we developed to employ interactive surface and volume rendering algorithms. Finally, we describe the results obtained, challenges that still need to be addressed, and ongoing efforts to meet the challenges of large-scale visualization.";2002;http://dx.doi.org/10.1109/VISUAL.2002.1183814;immersive volume seismic simulations case techniques invented lessons learned;documentation techniques invented results obtained lessons learned creating algorithms render outputs largescale seismic simulations objective development techniques collaborative simulation shared structural engineers seismologists computer scientists computer graphics research community witnessing large number exemplary publications addressing challenges faced trying visualize largescale surface volumetric datasets lately perspective issues like preprocessing simplification sampling filtering etc rendering algorithms surface volume interaction paradigms largescale highly interactive highly immersive etc light outline milestones achieved largescale simulation project opened scope combining techniques methods especially cases methods suitable elucidate simplification reorganization schemes discuss problems encountered solutions found desktop highend local well remote interfaces immersive systems employ interactive surface volume rendering algorithms finally results obtained challenges still need addressed ongoing efforts meet challenges largescale
Visibility-guided simplification;['visualization', 'mesh simplification'];For some graphics applications, object interiors and hard-to-see regions contribute little to the final images and need not be processed. In this paper, we define a view-independent visibility measure on mesh surfaces based on the visibility function between the surfaces and a surrounding sphere of cameras. We demonstrate the usefulness of this measure with a visibility-guided simplification algorithm. Mesh simplification reduces the polygon counts of 3D models and speeds up the rendering process. Many mesh simplification algorithms are based on sequences of edge collapses that minimize geometric and attribute errors. By combining the surface visibility measure with a geometric error measure, we obtain simplified models with improvement proportional to the number of low visibility regions in the original models.;2002;http://dx.doi.org/10.1109/VISUAL.2002.1183784;visibilityguided simplification;graphics applications object interiors hardtosee regions contribute little final images need processed define viewindependent visibility measure mesh surfaces based visibility function surfaces surrounding sphere cameras demonstrate usefulness measure visibilityguided simplification algorithm mesh simplification reduces polygon counts models speeds rendering process mesh simplification algorithms based sequences edge collapses minimize geometric attribute errors combining surface visibility measure geometric error measure obtain simplified models improvement proportional number low visibility regions original models
TetFusion: an algorithm for rapid tetrahedral mesh simplification;['level-of-detail', 'multi-resolution', 'mesh simplification'];This paper introduces an algorithm for rapid progressive simplification of tetrahedral meshes: TetFusion. We describe how a simple geometry decimation operation steers a rapid and controlled progressive simplification of tetrahedral meshes, while also taking care of complex mesh- inconsistency problems. The algorithm features a high decimation ratio per step, and inherently discourages any cases of self-intersection of boundary, element-boundary intersection at concave boundary-regions, and negative volume tetrahedra (flipping). We achieved rigorous reduction ratios of up to 98% for meshes consisting of 827,904 elements in less than 2 minutes, progressing through a series of level-of-details (LoDs) of the mesh in a controlled manner. We describe how the approach supports a balanced re-distribution of space between tetrahedral elements, and explain some useful control parameters that make it faster and more intuitive than 'edge collapse'-based decimation methods for volumetric meshes. Finally, we discuss how this approach can be employed for rapid LoD prototyping of large time-varying datasets as an aid to interactive visualization.;2002;http://dx.doi.org/10.1109/VISUAL.2002.1183767;tetfusion algorithm rapid tetrahedral mesh simplification;introduces algorithm rapid progressive simplification tetrahedral meshes tetfusion simple geometry decimation operation steers rapid controlled progressive simplification tetrahedral meshes taking care complex mesh inconsistency problems algorithm features high decimation ratio per step inherently discourages cases selfintersection boundary elementboundary intersection concave boundaryregions negative volume tetrahedra flipping achieved rigorous reduction ratios % meshes consisting elements less minutes progressing series levelofdetails lods mesh controlled manner supports balanced redistribution space tetrahedral elements explain useful control parameters make faster intuitive edge collapsebased decimation methods volumetric meshes finally discuss employed rapid lod prototyping large timevarying datasets aid interactive
Fast visualization of plane-like structures in voxel data;['visualization'];We present a robust, noise-resistant criterion characterizing plane-like skeletons in binary voxel objects. It is based on a distance map and the geodesic distance along the object's boundary. A parameter allows us to control the noise sensitivity. If needed, homotopy with the original object might be reconstructed in a second step, using an improved distance ordered thinning algorithm. The skeleton is analyzed to create a geometric representation for rendering. Plane-like parts are transformed into an triangulated surface not enclosing a volume by a suitable triangulation scheme. The resulting surfaces have lower triangle count than those created with standard methods and tend to maintain the original geometry, even after simplification with a high decimation rate. Our algorithm allows us to interactively render expressive images of complex 3D structures, emphasizing independently plane-like and rod-like structures. The methods are applied for visualization of the microstructure of bone biopsies.;2002;http://dx.doi.org/10.1109/VISUAL.2002.1183753;fast planelike structures voxel;robust noiseresistant criterion characterizing planelike skeletons binary voxel objects based distance map geodesic distance along objects boundary parameter allows control noise sensitivity needed homotopy original object might reconstructed second step improved distance ordered thinning algorithm skeleton analyzed create geometric representation rendering planelike parts transformed triangulated surface enclosing volume suitable triangulation scheme resulting surfaces lower triangle count created standard methods tend maintain original geometry even simplification high decimation rate algorithm allows interactively render expressive images complex structures emphasizing independently planelike rodlike structures methods applied microstructure bone biopsies
Hybrid simplification: combining multi-resolution polygon and point rendering;['multi-resolution', 'simplification'];Multi-resolution hierarchies of polygons and more recently of points are familiar and useful tools for achieving interactive rendering rates. We present an algorithm for tightly integrating the two into a single hierarchical data structure. The trade-off between rendering portions of a model with points or with polygons is made automatically. Our approach to this problem is to apply a bottom-up simplification process involving not only polygon simplification operations, but point replacement and point simplification operations as well. Given one or more surface meshes, our algorithm produces a hybrid hierarchy comprising both polygon and point primitives. This hierarchy may be optimized according to the relative performance characteristics of these primitive types on the intended rendering platform. We also provide a range of aggressiveness for performing point replacement operations. The most conservative approach produces a hierarchy that is better than a purely polygonal hierarchy in some places, and roughly equal in others. A less conservative approach can trade reduced complexity at the far viewing ranges for some increased complexity at the near viewing ranges. We demonstrate our approach on a number of input models, achieving primitive counts that are 1.3 to 4.7 times smaller than those of triangle-only simplification.;2001;http://dx.doi.org/10.1109/VISUAL.2001.964491;hybrid simplification combining multiresolution polygon point rendering;multiresolution hierarchies polygons recently points familiar useful tools achieving interactive rendering rates algorithm tightly integrating two single hierarchical structure tradeoff rendering portions model points polygons made automatically bottomup simplification process involving polygon simplification operations point replacement point simplification operations well one surface meshes algorithm produces hybrid hierarchy comprising polygon point primitives hierarchy may optimized according relative performance characteristics primitive types intended rendering platform provide range aggressiveness performing point replacement operations conservative produces hierarchy purely polygonal hierarchy places roughly equal others less conservative trade reduced complexity far viewing ranges increased complexity near viewing ranges demonstrate number input models achieving primitive counts times smaller triangleonly simplification
Multiresolution feature extraction for unstructured meshes;['feature extraction', 'multi-resolution model'];We present a framework to extract mesh features from unstructured two-manifold surfaces. Our method computes a collection of piecewise linear curves describing the salient features of surfaces, such as edges and ridge lines. We extend these basic techniques to a multiresolution setting which improves the quality of the results and accelerates the extraction process. The extraction process is semi-automatic, that is, the user is required to input a few control parameters and to select the operators to be applied to the input surface. Our mesh feature extraction algorithm can be used as a preprocessor for a variety of applications in geometric modeling including mesh fairing, subdivision and simplification.;2001;http://dx.doi.org/10.1109/VISUAL.2001.964523;multiresolution feature extraction unstructured meshes;framework extract mesh features unstructured twomanifold surfaces method computes collection piecewise linear curves describing salient features surfaces edges ridge lines extend basic techniques multiresolution setting improves quality results accelerates extraction process extraction process semiautomatic user required input control parameters select operators applied input surface mesh feature extraction algorithm preprocessor variety applications geometric modeling including mesh fairing subdivision simplification
Continuous topology simplification of planar vector fields;['simplification'];Vector fields can present complex structural behavior, especially in turbulent computational fluid dynamics. The topological analysis of these data sets reduces the information, but one is usually still left with too many details for interpretation. In this paper, we present a simplification approach that removes pairs of critical points from the data set, based on relevance measures. In contrast to earlier methods, no grid changes are necessary, since the whole method uses small local changes of the vector values defining the vector field. An interpretation in terms of bifurcations underlines the continuous, natural flavor of the algorithm.;2001;http://dx.doi.org/10.1109/VISUAL.2001.964507;continuous topology simplification planar vector fields;vector fields complex structural behavior especially turbulent computational fluid dynamics topological analysis sets reduces information one usually still left details interpretation simplification removes pairs critical points set based relevance measures contrast earlier methods grid changes necessary since whole method uses small local changes vector values defining vector field interpretation terms bifurcations underlines continuous natural flavor algorithm
Attribute preserving dataset simplification;['mesh simplification'];"The paper describes a novel application of feature preserving mesh simplification to the problem of managing large, multidimensional datasets during scientific visualization. To allow this, we view a scientific dataset as a triangulated mesh of data elements, where the attributes embedded in each element form a set of properties arrayed across the surface of the mesh. Existing simplification techniques were not designed to address the high dimensionality that exists in these types of datasets. In addition, vertex operations that relocate, insert, or remove data elements may need to be modified or restricted. Principal component analysis provides an algorithm-independent method for compressing a dataset's dimensionality during simplification. Vertex locking forces certain data elements to maintain their spatial locations; this technique is also used to guarantee a minimum density in the simplified dataset. The result is a visualization that significantly reduces the number of data elements to display, while at the same time ensuring that high-variance regions of potential interest remain intact. We apply our techniques to a number of well-known feature preserving algorithms, and demonstrate their applicability in a real-world context by simplifying a multidimensional weather dataset. Our results show a significant improvement in execution time with only a small reduction in accuracy; even when the dataset was simplified to 10% of its original size, average per attribute error was less than 1%.";2001;http://dx.doi.org/10.1109/VISUAL.2001.964501;attribute preserving dataset simplification;describes application feature preserving mesh simplification managing large multidimensional datasets scientific allow view scientific dataset triangulated mesh elements attributes embedded element form set properties arrayed across surface mesh simplification techniques designed high dimensionality exists types datasets addition vertex operations relocate insert remove elements may need modified restricted principal component analysis provides algorithmindependent method compressing datasets dimensionality simplification vertex locking forces certain elements maintain spatial locations technique guarantee minimum density simplified dataset result significantly reduces number elements display time ensuring highvariance regions potential interest remain intact techniques number wellknown feature preserving algorithms demonstrate applicability realworld context simplifying multidimensional weather dataset results significant improvement execution time small reduction accuracy even dataset simplified % original size average per attribute error less %
Simplification of tetrahedral meshes with accurate error evaluation;['mesh simplification'];The techniques for reducing the size of a volume dataset by preserving both the geometrical/topological shape and the information encoded in an attached scalar field are attracting growing interest. Given the framework of incremental 3D mesh simplification based on edge collapse, we propose an approach for the integrated evaluation of the error introduced by both the modification of the domain and the approximation of the field of the original volume dataset. We present and compare various techniques to evaluate the approximation error or to produce a sound prediction. A flexible simplification tool has been implemented, which provides a different degree of accuracy and computational efficiency for the selection of the edge to be collapsed. Techniques for preventing a geometric or topological degeneration of the mesh are also presented.;2000;http://dx.doi.org/10.1109/VISUAL.2000.885680;simplification tetrahedral meshes accurate error evaluation;techniques reducing size volume dataset preserving geometricaltopological shape information encoded attached scalar field attracting growing interest framework incremental mesh simplification based edge collapse propose integrated evaluation error introduced modification domain approximation field original volume dataset compare various techniques evaluate approximation error produce sound prediction flexible simplification tool implemented provides different degree accuracy computational efficiency selection edge collapsed techniques preventing geometric topological degeneration mesh presented
A topology simplification method for 2D vector fields;['flowvisualization', 'simplification'];Topology analysis of plane, turbulent vector fields results in visual clutter caused by critical points indicating vortices of finer and finer scales. A simplification can be achieved by merging critical points within a prescribed radius into higher order critical points. After building clusters containing the singularities to merge, the method generates a piecewise linear representation of the vector field in each cluster containing only one (higher order) singularity. Any visualization method can be applied to the result after this process. Using different maximal distances for the critical points to be merged results in a hierarchy of simplified vector fields that can be used for analysis on different scales.;2000;http://dx.doi.org/10.1109/VISUAL.2000.885716;topology simplification method vector fields;topology analysis plane turbulent vector fields results clutter caused critical points indicating vortices finer finer scales simplification achieved merging critical points prescribed radius higher order critical points building clusters containing singularities merge method generates piecewise linear representation vector field cluster containing one higher order singularity method applied result process different maximal distances critical points merged results hierarchy simplified vector fields analysis different scales
Simplification of surface annotations;['simplification'];Geometric models are often annotated to provide additional information during visualization. Maps may be marked with rivers, roads or topographical information, and CAD data models may highlight the underlying mesh structure. While this additional information may be extremely useful, there is a rendering cost associated with it. Texture maps have often been used to convey this information at relatively low cost, but they suffer from blurring and pixelization at high magnification. We present a technique for simplifying surface annotations based on directed, asymmetric tolerance. By maintaining the annotations as geometry, as opposed to textures, we are able to simplify them while still maintaining the overall appearance of the model over a wide range of magnifications. Texture maps may still be used to provide low-resolution surface detail, such as color. We demonstrate a significant gain in rendering performance while retaining the original appearance of objects from many application domains.;2000;http://dx.doi.org/10.1109/VISUAL.2000.885700;simplification surface annotations;geometric models often annotated provide additional information maps may marked rivers roads topographical information cad models may highlight underlying mesh structure additional information may extremely useful rendering cost associated texture maps often convey information relatively low cost suffer blurring pixelization high magnification technique simplifying surface annotations based directed asymmetric tolerance maintaining annotations geometry opposed textures able simplify still maintaining overall appearance model wide range magnifications texture maps may still provide lowresolution surface detail color demonstrate significant gain rendering performance retaining original appearance objects application domains
Simplified representation of vector fields;['flowvisualization', 'simplification'];Vector field visualization remains a difficult task. Many local and global visualization methods for vector fields such as flow data exist, but they usually require extensive user experience on setting the visualization parameters in order to produce images communicating the desired insight. We present a visualization method that produces simplified but suggestive images of the vector field automatically, based on a hierarchical clustering of the input data. The resulting clusters are then visualized with straight or curved arrow icons. The presented method has a few parameters with which users can produce various simplified vector field visualizations that communicate different insights on the vector data.;1999;http://dx.doi.org/10.1109/VISUAL.1999.809865;simplified representation vector fields;vector field remains difficult task local global methods vector fields flow exist usually require extensive user experience setting parameters order produce images communicating desired insight method produces simplified suggestive images vector field automatically based hierarchical clustering input resulting clusters visualized straight curved arrow icons presented method parameters users produce various simplified vector field visualizations communicate different insights vector
Progressive tetrahedralizations;['level-of-detail', 'multi-resolution', 'mesh simplification'];The paper describes some fundamental issues for robust implementations of progressively refined tetrahedralizations generated through sequences of edge collapses. We address the definition of appropriate cost functions and explain on various tests which are necessary to preserve the consistency of the mesh when collapsing edges. Although considered a special case of progressive simplicial complexes (J. Popovic and H. Hoppe, 1997), the results of our method are of high practical importance and can be used in many different applications, such as finite element meshing, scattered data interpolation, or rendering of unstructured volume data.;1998;http://dx.doi.org/10.1109/VISUAL.1998.745329;progressive tetrahedralizations;describes fundamental issues robust implementations progressively refined tetrahedralizations generated sequences edge collapses definition appropriate cost functions explain various tests necessary preserve consistency mesh collapsing edges although considered special case progressive simplicial complexes j popovic h hoppe results method high practical importance different applications finite element meshing scattered interpolation rendering unstructured volume
A unified approach for simplifying polygonal and spline models;['level-of-detail'];We present a new approach for simplifying models composed of polygons or spline patches. Given an input model, the algorithm computes a new representation of the model in terms of triangular Bezier patches. It performs a series of geometric operations, consisting of patch merging and swapping diagonals, and makes use of batch connectivity information to generate C-LODs (curved levels-of-detail). Each C-LOD is represented using cubic triangular Bezier patches. The C-LODs provide a compact representation for storing the model. The algorithm tries to minimize the surface deviation error and maintains continuity at patch boundaries. Given the CLODs, the algorithm can generate their polygonal approximations using static and dynamic tessellation schemes. It has been implemented and we highlight its performance on a number of polygonal and spline models.;1998;http://dx.doi.org/10.1109/VISUAL.1998.745313;unified simplifying polygonal spline models;simplifying models composed polygons spline patches input model algorithm computes representation model terms triangular bezier patches performs series geometric operations consisting patch merging swapping diagonals makes use batch connectivity information generate clods curved levelsofdetail clod represented cubic triangular bezier patches clods provide compact representation storing model algorithm tries minimize surface deviation error maintains continuity patch boundaries clods algorithm generate polygonal approximations static dynamic tessellation schemes implemented highlight performance number polygonal spline models
Simplifying surfaces with color and texture using quadric error metrics;['level-of-detail'];There are a variety of application areas in which there is a need for simplifying complex polygonal surface models. These models often have material properties such as colors, textures, and surface normals. Our surface simplification algorithm, based on iterative edge contraction and quadric error metrics, can rapidly produce high quality approximations of such models. We present a natural extension of our original error metric that can account for a wide range of vertex attributes.;1998;http://dx.doi.org/10.1109/VISUAL.1998.745312;simplifying surfaces color texture quadric error metrics;variety application need simplifying complex polygonal surface models models often material properties colors textures surface normals surface simplification algorithm based iterative edge contraction quadric error metrics rapidly produce high quality approximations models natural extension original error metric account wide range vertex attributes
Simplifying polygonal models using successive mappings;['level-of-detail'];We present the use of mapping functions to automatically generate levels of detail with known error bounds for polygonal models. We develop a piece-wise linear mapping function for each simplification operation and use this function to measure deviation of the new surface from both the previous level of detail and from the original surface. In addition, we use the mapping function to compute appropriate texture coordinates if the original map has texture coordinates at its vertices. Our overall algorithm uses edge collapse operations. We present rigorous procedures for the generation of local planar projections as well as for the selection of a new vertex position for the edge collapse operation. As compared to earlier methods, our algorithm is able to compute tight error bounds on surface deviation and produce an entire continuum of levels of detail with mappings between them. We demonstrate the effectiveness of our algorithm on several models: a Ford Bronco consisting of over 300 parts and 70,000 triangles, a textured lion model consisting of 49 parts and 86,000 triangles, and a textured, wrinkled torus consisting of 79,000 triangles.;1997;http://dx.doi.org/10.1109/VISUAL.1997.663908;simplifying polygonal models successive mappings;use mapping functions automatically generate levels detail known error bounds polygonal models develop piecewise linear mapping function simplification operation use function measure deviation surface level detail original surface addition use mapping function compute appropriate texture coordinates original map texture coordinates vertices overall algorithm uses edge collapse operations rigorous procedures generation local planar projections well selection vertex position edge collapse operation compared earlier methods algorithm able compute tight error bounds surface deviation produce entire continuum levels detail mappings demonstrate effectiveness algorithm several models ford bronco consisting parts triangles textured lion model consisting parts triangles textured wrinkled torus consisting triangles
LISTEN: sounding uncertainty visualization;['visualization'];"Integrated presentation of data with uncertainty is a worthy goal in scientific visualization. It allows researchers to make informed decisions based on imperfect data. It also allows users to visually compare and contrast different algorithms for performing the same task or different models for representing the same physical phenomenon. We present LISTEN-a data sonification system that has been incorporated into two visualization systems: a system for visualizing geometric uncertainty of surface interpolants; and a system for visualizing uncertainty in fluid flow. LISTEN is written in C++ for the SGI platform. It works with the SGI internal audio chip or a MIDI device or both. LISTEN is an object-oriented system that is modular, flexible, adaptable, portable, interactive and extensible. We demonstrate that sonification is very effective as an additional tool in visualizing geometric and fluid flow uncertainty.";1996;http://dx.doi.org/10.1109/VISUAL.1996.568105;listen sounding uncertainty;integrated presentation uncertainty worthy scientific allows researchers make informed decisions based imperfect allows users visually compare contrast different algorithms performing task different models representing physical phenomenon listena sonification system incorporated two systems system geometric uncertainty surface interpolants system uncertainty fluid flow listen written c++ sgi platform works sgi internal audio chip midi device listen objectoriented system modular flexible adaptable portable interactive extensible demonstrate sonification effective additional tool geometric fluid flow uncertainty
Adaptive Multilinear Tensor Product Wavelets;['multi-resolution model'];Many foundational visualization techniques including isosurfacing, direct volume rendering and texture mapping rely on piecewise multilinear interpolation over the cells of a mesh. However, there has not been much focus within the visualization community on techniques that efficiently generate and encode globally continuous functions defined by the union of multilinear cells. Wavelets provide a rich context for analyzing and processing complicated datasets. In this paper, we exploit adaptive regular refinement as a means of representing and evaluating functions described by a subset of their nonzero wavelet coefficients. We analyze the dependencies involved in the wavelet transform and describe how to generate and represent the coarsest adaptive mesh with nodal function values such that the inverse wavelet transform is exactly reproduced via simple interpolation (subdivision) over the mesh elements. This allows for an adaptive, sparse representation of the function with on-demand evaluation at any point in the domain. We focus on the popular wavelets formed by tensor products of linear B-splines, resulting in an adaptive, nonconforming but crack-free quadtree (2D) or octree (3D) mesh that allows reproducing globally continuous functions via multilinear interpolation over its cells.;2015;http://dx.doi.org/10.1109/TVCG.2015.2467412;adaptive multilinear tensor product wavelets;foundational techniques including isosurfacing direct volume rendering texture mapping rely piecewise multilinear interpolation cells mesh however focus community techniques efficiently generate encode globally continuous functions defined union multilinear cells wavelets provide rich context processing complicated datasets exploit adaptive regular refinement means representing evaluating functions described subset nonzero wavelet coefficients analyze dependencies involved wavelet transform generate represent coarsest adaptive mesh nodal function values inverse wavelet transform exactly reproduced simple interpolation subdivision mesh elements allows adaptive sparse representation function ondemand evaluation point domain focus popular wavelets formed tensor products linear bsplines resulting adaptive nonconforming crackfree quadtree octree mesh allows reproducing globally continuous functions multilinear interpolation cells
Supercubes: A High-Level Primitive for Diamond Hierarchies;['multi-resolution model'];Volumetric datasets are often modeled using a multiresolution approach based on a nested decomposition of the domain into a polyhedral mesh. Nested tetrahedral meshes generated through the longest edge bisection rule are commonly used to decompose regular volumetric datasets since they produce highly adaptive crack-free representations. Efficient representations for such models have been achieved by clustering the set of tetrahedra sharing a common longest edge into a structure called a diamond. The alignment and orientation of the longest edge can be used to implicitly determine the geometry of a diamond and its relations to the other diamonds within the hierarchy. We introduce the supercube as a high-level primitive within such meshes that encompasses all unique types of diamonds. A supercube is a coherent set of edges corresponding to three consecutive levels of subdivision. Diamonds are uniquely characterized by the longest edge of the tetrahedra forming them and are clustered in supercubes through the association of the longest edge of a diamond with a unique edge in a supercube. Supercubes are thus a compact and highly efficient means of associating information with a subset of the vertices, edges and tetrahedra of the meshes generated through longest edge bisection. We demonstrate the effectiveness of the supercube representation when encoding multiresolution diamond hierarchies built on a subset of the points of a regular grid. We also show how supercubes can be used to efficiently extract meshes from diamond hierarchies and to reduce the storage requirements of such variable-resolution meshes.;2009;http://dx.doi.org/10.1109/TVCG.2009.186;supercubes highlevel primitive diamond hierarchies;volumetric datasets often modeled multiresolution based nested decomposition domain polyhedral mesh nested tetrahedral meshes generated longest edge bisection rule commonly decompose regular volumetric datasets since produce highly adaptive crackfree representations efficient representations models achieved clustering set tetrahedra sharing longest edge structure called diamond alignment orientation longest edge implicitly determine geometry diamond relations diamonds hierarchy supercube highlevel primitive meshes encompasses unique types diamonds supercube coherent set edges corresponding three consecutive levels subdivision diamonds uniquely characterized longest edge tetrahedra forming clustered supercubes association longest edge diamond unique edge supercube supercubes thus compact highly efficient means associating information subset vertices edges tetrahedra meshes generated longest edge bisection demonstrate effectiveness supercube representation encoding multiresolution diamond hierarchies built subset points regular grid supercubes efficiently extract meshes diamond hierarchies reduce storage requirements variableresolution meshes
Real-time refinement and simplification of adaptive triangular meshes;['level-of-detail', 'multi-resolution'];In this paper we present a generic method for incremental mesh adaptation based on hierarchy of semi-regular meshes. Our method supports any refinement rule mapping vertices onto vertices such as 1-to-4 split or /spl radic/3-subdivision. Resulting adaptive mesh has subdivision connectivity and hence good aspect ratio of triangles. Hierarchic representation of the mesh allows incremental local refinement and simplification operations exploiting frame-to-frame coherence. We also present an out-of-core storage layout scheme designed for semi-regular meshes of arbitrary subdivision connectivity. It provides high cache coherency in the data retrieval and relies on the interleaved storage of resolution levels and maintaining good geometrical proximity within each level. The efficiency of the proposed method is demonstrated with applications in physically-based cloth simulation, real-time terrain visualization and procedural modeling.;2003;http://dx.doi.org/10.1109/VISUAL.2003.1250367;realtime refinement simplification adaptive triangular meshes;generic method incremental mesh adaptation based hierarchy semiregular meshes method supports refinement rule mapping vertices onto vertices split spl radicsubdivision resulting adaptive mesh subdivision connectivity hence good aspect ratio triangles hierarchic representation mesh allows incremental local refinement simplification operations exploiting frametoframe coherence outofcore storage layout scheme designed semiregular meshes arbitrary subdivision connectivity provides high cache coherency retrieval relies interleaved storage resolution levels maintaining good geometrical proximity level efficiency proposed method demonstrated applications physicallybased cloth simulation realtime terrain procedural modeling
Implant sprays: compression of progressive tetrahedral mesh connectivity;['multi-resolution model'];"Irregular tetrahedral meshes, which are popular in many engineering and scientific applications, often contain a large number of vertices. A mesh of V vertices and T tetrahedra requires 48 V bits or less to store the vertex coordinates, 4/spl middot/T/spl middot/log/sub 2/(V) bits to store the tetrahedra-vertex incidence relations, also called connectivity information, and kV bits to store the k-bit value samples associated with the vertices. Given that T is 5 to 7 times larger than V and that V often exceeds 32/sup 3/, the storage space required for the connectivity is larger than 300 V bits and thus dominates the overall storage cost. Our \""implants spray\"" compression approach introduced in the paper reduces this cost to about 30 V bits or less-a 10:1 compression ratio. Furthermore, implant spray supports the progressive refinement of a crude model through a series of vertex-splits operations.";1999;http://dx.doi.org/10.1109/VISUAL.1999.809901;implant sprays compression progressive tetrahedral mesh connectivity;irregular tetrahedral meshes popular engineering scientific applications often large number vertices mesh v vertices tetrahedra requires v bits less store vertex coordinates spl middottspl middotlogsub v bits store tetrahedravertex incidence relations called connectivity information kv bits store kbit value samples associated vertices times larger v v often exceeds storage space required connectivity larger v bits thus dominates overall storage cost implants spray compression introduced reduces cost v bits lessa compression ratio furthermore implant spray supports progressive refinement crude model series vertexsplits operations
Simplification of tetrahedral meshes;['visualization'];We present a method for the construction of multiple levels of tetrahedral meshes approximating a trivariate function at different levels of detail. Starting with an initial, high-resolution triangulation of a three-dimensional region, we construct coarser representation levels by collapsing tetrahedra. Each triangulation defines a linear spline function, where the function values associated with the vertices are the spline coefficients. Based on predicted errors, we collapse tetrahedron in the grid that do not cause the maximum error to exceed a use-specified threshold. Bounds are stored for individual tetrahedra and are updated as the mesh is simplified. We continue the simplification process until a certain error is reached. The result is a hierarchical data description suited for the efficient visualization of large data sets at varying levels of detail.;1998;http://dx.doi.org/10.1109/VISUAL.1998.745315;simplification tetrahedral meshes;method construction multiple levels tetrahedral meshes approximating trivariate function different levels detail starting initial highresolution triangulation threedimensional region construct coarser representation levels collapsing tetrahedra triangulation defines linear spline function function values associated vertices spline coefficients based predicted errors collapse tetrahedron grid cause maximum error exceed usespecified threshold bounds stored individual tetrahedra updated mesh simplified continue simplification process certain error reached result hierarchical description suited efficient large sets varying levels detail
Interpolation of triangle hierarchies;['level-of-detail'];We consider interpolation between keyframe hierarchies. We impose a set of weak constraints that allows smooth interpolation between two keyframe hierarchies in an animation or, more generally, allows the interpolation in an n-parameter family of hierarchies. We use hierarchical triangulations obtained by the Rivara element bisection algorithm (M. Rivara, 1984) and impose a weak compatibility constraint on the set of root elements of all keyframe hierarchies. We show that the introduced constraints are rather weak. The strength of our approach is that the interpolation works in the class of conforming triangulations and simplifies the task of finding the intermediate hierarchy, which is the union of the two (or more) keyframe hierarchies involved in the interpolation process. This allows for an efficient generation of the intermediate connectivity and additionally ensures that the intermediate hierarchy is again a conforming hierarchy satisfying the same constraints.;1998;http://dx.doi.org/10.1109/VISUAL.1998.745328;interpolation triangle hierarchies;consider interpolation keyframe hierarchies impose set weak constraints allows smooth interpolation two keyframe hierarchies animation allows interpolation nparameter family hierarchies use hierarchical triangulations obtained rivara element bisection algorithm rivara impose weak compatibility constraint set root elements keyframe hierarchies introduced constraints rather weak strength interpolation works class conforming triangulations simplifies task finding intermediate hierarchy union two keyframe hierarchies involved interpolation process allows efficient generation intermediate connectivity additionally ensures intermediate hierarchy conforming hierarchy satisfying constraints
In Situ Eddy Analysis in a High-Resolution Ocean Climate Model;['feature extraction'];An eddy is a feature associated with a rotating body of fluid, surrounded by a ring of shearing fluid. In the ocean, eddies are 10 to 150 km in diameter, are spawned by boundary currents and baroclinic instabilities, may live for hundreds of days, and travel for hundreds of kilometers. Eddies are important in climate studies because they transport heat, salt, and nutrients through the world's oceans and are vessels of biological productivity. The study of eddies in global ocean-climate models requires large-scale, high-resolution simulations. This poses a problem for feasible (timely) eddy analysis, as ocean simulations generate massive amounts of data, causing a bottleneck for traditional analysis workflows. To enable eddy studies, we have developed an in situ workflow for the quantitative and qualitative analysis of MPAS-Ocean, a high- resolution ocean climate model, in collaboration with the ocean model research and development process. Planned eddy analysis at high spatial and temporal resolutions will not be possible with a postprocessing workflow due to various constraints, such as storage size and I/O time, but the in situ workflow enables it and scales well to ten-thousand processing elements.;2015;http://dx.doi.org/10.1109/TVCG.2015.2467411;situ eddy analysis highresolution ocean climate model;eddy feature associated rotating body fluid surrounded ring shearing fluid ocean eddies km diameter spawned boundary currents baroclinic instabilities may live hundreds days travel hundreds kilometers eddies important climate transport heat salt nutrients worlds oceans vessels biological productivity eddies global oceanclimate models requires largescale highresolution simulations poses feasible timely eddy analysis ocean simulations generate massive amounts causing bottleneck traditional analysis workflows enable eddy situ workflow quantitative qualitative analysis mpasocean high resolution ocean climate model collaboration ocean model research development process planned eddy analysis high spatial temporal resolutions possible postprocessing workflow due various constraints storage size io time situ workflow enables scales well tenthousand processing elements
Trajectory-Based Flow Feature Tracking in Joint Particle/Volume Datasets;['flowvisualization'];Studying the dynamic evolution of time-varying volumetric data is essential in countless scientific endeavors. The ability to isolate and track features of interest allows domain scientists to better manage large complex datasets both in terms of visual understanding and computational efficiency. This work presents a new trajectory-based feature tracking technique for use in joint particle/volume datasets. While traditional feature tracking approaches generally require a high temporal resolution, this method utilizes the indexed trajectories of corresponding Lagrangian particle data to efficiently track features over large jumps in time. Such a technique is especially useful for situations where the volume dataset is either temporally sparse or too large to efficiently track a feature through all intermediate timesteps. In addition, this paper presents a few other applications of this approach, such as the ability to efficiently track the internal properties of volumetric features using variables from the particle data. We demonstrate the effectiveness of this technique using real world combustion and atmospheric datasets and compare it to existing tracking methods to justify its advantages and accuracy.;2014;http://dx.doi.org/10.1109/TVCG.2014.2346423;trajectorybased flow feature tracking joint particlevolume datasets;studying dynamic evolution timevarying volumetric essential countless scientific endeavors ability isolate track features interest allows domain scientists manage large complex datasets terms understanding computational efficiency presents trajectorybased feature tracking technique use joint particlevolume datasets traditional feature tracking approaches require high temporal resolution method utilizes indexed trajectories corresponding lagrangian particle efficiently track features large jumps time technique especially useful situations volume dataset either temporally sparse large efficiently track feature intermediate timesteps addition presents applications ability efficiently track internal properties volumetric features variables particle demonstrate effectiveness technique real world combustion atmospheric datasets compare tracking methods justify advantages accuracy
Analysis of Streamline Separation at Infinity Using Time-Discrete Markov Chains;['feature extraction'];Existing methods for analyzing separation of streamlines are often restricted to a finite time or a local area. In our paper we introduce a new method that complements them by allowing an infinite-time- evaluation of steady planar vector fields. Our algorithm unifies combinatorial and probabilistic methods and introduces the concept of separation in time- discrete Markov-Chains. We compute particle distributions instead of the streamlines of single particles. We encode the flow into a map and then into a transition matrix for each time direction. Finally, we compare the results of our grid-independent algorithm to the popular Finite-Time-Lyapunov-Exponents and discuss the discrepancies.;2012;http://dx.doi.org/10.1109/TVCG.2012.198;analysis streamline separation infinity timediscrete markov chains;methods separation streamlines often restricted finite time local area method complements allowing infinitetime evaluation steady planar vector fields algorithm unifies combinatorial probabilistic methods introduces concept separation time discrete markovchains compute particle distributions instead streamlines single particles encode flow map transition matrix time direction finally compare results gridindependent algorithm popular finitetimelyapunovexponents discuss discrepancies
Adaptive Extraction and Quantification of Geophysical Vortices;['feature extraction'];We consider the problem of extracting discrete two-dimensional vortices from a turbulent flow. In our approach we use a reference model describing the expected physics and geometry of an idealized vortex. The model allows us to derive a novel correlation between the size of the vortex and its strength, measured as the square of its strain minus the square of its vorticity. For vortex detection in real models we use the strength parameter to locate potential vortex cores, then measure the similarity of our ideal analytical vortex and the real vortex core for different strength thresholds. This approach provides a metric for how well a vortex core is modeled by an ideal vortex. Moreover, this provides insight into the problem of choosing the thresholds that identify a vortex. By selecting a target coefficient of determination (i.e., statistical confidence), we determine on a per-vortex basis what threshold of the strength parameter would be required to extract that vortex at the chosen confidence. We validate our approach on real data from a global ocean simulation and derive from it a map of expected vortex strengths over the global ocean.;2011;http://dx.doi.org/10.1109/TVCG.2011.162;adaptive extraction quantification geophysical vortices;consider extracting discrete twodimensional vortices turbulent flow use reference model describing expected physics geometry idealized vortex model allows derive correlation size vortex strength measured square strain minus square vorticity vortex detection real models use strength parameter locate potential vortex cores measure similarity ideal analytical vortex real vortex core different strength thresholds provides metric well vortex core modeled ideal vortex moreover provides insight choosing thresholds identify vortex selecting target coefficient determination ie statistical confidence determine pervortex threshold strength parameter would required extract vortex chosen confidence validate real global ocean simulation derive map expected vortex strengths global ocean
Two-Dimensional Time-Dependent Vortex Regions Based on the Acceleration Magnitude;['feature extraction'];Acceleration is a fundamental quantity of flow fields that captures Galilean invariant properties of particle motion. Considering the magnitude of this field, minima represent characteristic structures of the flow that can be classified as saddle- or vortex-like. We made the interesting observation that vortex-like minima are enclosed by particularly pronounced ridges. This makes it possible to define boundaries of vortex regions in a parameter-free way. Utilizing scalar field topology, a robust algorithm can be designed to extract such boundaries. They can be arbitrarily shaped. An efficient tracking algorithm allows us to display the temporal evolution of vortices. Various vortex models are used to evaluate the method. We apply our method to two-dimensional model systems from computational fluid dynamics and compare the results to those arising from existing definitions.;2011;http://dx.doi.org/10.1109/TVCG.2011.249;twodimensional timedependent vortex regions based acceleration magnitude;acceleration fundamental quantity flow fields captures galilean invariant properties particle motion considering magnitude field minima represent characteristic structures flow classified saddle vortexlike made interesting observation vortexlike minima enclosed particularly pronounced ridges makes possible define boundaries vortex regions parameterfree way utilizing scalar field topology robust algorithm designed extract boundaries arbitrarily shaped efficient tracking algorithm allows display temporal evolution vortices various vortex models evaluate method method twodimensional model systems computational fluid dynamics compare results arising definitions
Predictor-Corrector Schemes for Visualization ofSmoothed Particle Hydrodynamics Data;['flowvisualization', 'feature extraction'];In this paper we present a method for vortex core line extraction which operates directly on the smoothed particle hydrodynamics (SPH) representation and, by this, generates smoother and more (spatially and temporally) coherent results in an efficient way. The underlying predictor-corrector scheme is general enough to be applied to other line-type features and it is extendable to the extraction of surfaces such as isosurfaces or Lagrangian coherent structures. The proposed method exploits temporal coherence to speed up computation for subsequent time steps. We show how the predictor-corrector formulation can be specialized for several variants of vortex core line definitions including two recent unsteady extensions, and we contribute a theoretical and practical comparison of these. In particular, we reveal a close relation between unsteady extensions of Fuchs et al. and Weinkauf et al. and we give a proof of the Galilean invariance of the latter. When visualizing SPH data, there is the possibility to use the same interpolation method for visualization as has been used for the simulation. This is different from the case of finite volume simulation results, where it is not possible to recover from the results the spatial interpolation that was used during the simulation. Such data are typically interpolated using the basic trilinear interpolant, and if smoothness is required, some artificial processing is added. In SPH data, however, the smoothing kernels are specified from the simulation, and they provide an exact and smooth interpolation of data or gradients at arbitrary points in the domain.;2009;http://dx.doi.org/10.1109/TVCG.2009.173;predictorcorrector schemes ofsmoothed particle hydrodynamics;method vortex core line extraction operates directly smoothed particle hydrodynamics sph representation generates smoother spatially temporally coherent results efficient way underlying predictorcorrector scheme general enough applied linetype features extendable extraction surfaces isosurfaces lagrangian coherent structures proposed method exploits temporal coherence speed computation subsequent time steps predictorcorrector formulation specialized several variants vortex core line definitions including two unsteady extensions contribute theoretical practical comparison particular reveal close relation unsteady extensions fuchs et al weinkauf et al give proof galilean invariance latter sph possibility use interpolation method simulation different case finite volume simulation results possible recover results spatial interpolation simulation typically interpolated basic trilinear interpolant smoothness required artificial processing added sph however smoothing kernels specified simulation provide exact smooth interpolation gradients arbitrary points domain
Texture-based feature tracking for effective time-varying data visualization;['visualization'];Analyzing, visualizing, and illustrating changes within time-varying volumetric data is challenging due to the dynamic changes occurring between timesteps. The changes and variations in computational fluid dynamic volumes and atmospheric 3D datasets do not follow any particular transformation. Features within the data move at different speeds and directions making the tracking and visualization of these features a difficult task. We introduce a texture-based feature tracking technique to overcome some of the current limitations found in the illustration and visualization of dynamic changes within time-varying volumetric data. Our texture-based technique tracks various features individually and then uses the tracked objects to better visualize structural changes. We show the effectiveness of our texture-based tracking technique with both synthetic and real world time-varying data. Furthermore, we highlight the specific visualization, annotation, registration, and feature isolation benefits of our technique. For instance, we show how our texture- based tracking can lead to insightful visualizations of time-varying data. Such visualizations, more than traditional visualization techniques, can assist domain scientists to explore and understand dynamic changes.;2007;http://dx.doi.org/10.1109/TVCG.2007.70599;texturebased feature tracking effective timevarying;illustrating changes timevarying volumetric challenging due dynamic changes occurring timesteps changes variations computational fluid dynamic volumes atmospheric datasets follow particular transformation features move different speeds directions making tracking features difficult task texturebased feature tracking technique overcome current limitations found illustration dynamic changes timevarying volumetric texturebased technique tracks various features individually uses tracked objects visualize structural changes effectiveness texturebased tracking technique synthetic real world timevarying furthermore highlight specific annotation registration feature isolation benefits technique instance texture based tracking lead insightful visualizations timevarying visualizations traditional techniques assist domain scientists explore dynamic changes
Efficient Computation and Visualization of Coherent Structures in Fluid Flow Applications;['flowvisualization', 'feature detection'];The recently introduced notion of Finite-Time Lyapunov Exponent to characterize Coherent Lagrangian Structures provides a powerful framework for the visualization and analysis of complex technical flows. Its definition is simple and intuitive, and it has a deep theoretical foundation. While the application of this approach seems straightforward in theory, the associated computational cost is essentially prohibitive. Due to the Lagrangian nature of this technique, a huge number of particle paths must be computed to fill the space-time flow domain. In this paper, we propose a novel scheme for the adaptive computation of FTLE fields in two and three dimensions that significantly reduces the number of required particle paths. Furthermore, for three-dimensional flows, we show on several examples that meaningful results can be obtained by restricting the analysis to a well-chosen plane intersecting the flow domain. Finally, we examine some of the visualization aspects of FTLE-based methods and introduce several new variations that help in the analysis of specific aspects of a flow.;2007;http://dx.doi.org/10.1109/TVCG.2007.70551;efficient computation coherent structures fluid flow applications;recently introduced notion finitetime lyapunov exponent characterize coherent lagrangian structures provides powerful framework analysis complex technical flows definition simple intuitive deep theoretical foundation application seems straightforward theory associated computational cost essentially prohibitive due lagrangian nature technique huge number particle paths must computed fill spacetime flow domain propose scheme adaptive computation ftle fields two three dimensions significantly reduces number required particle paths furthermore threedimensional flows several examples meaningful results obtained restricting analysis wellchosen plane intersecting flow domain finally examine aspects ftlebased methods several variations help analysis specific aspects flow
Efficient Computation of Morse-Smale Complexes for Three-dimensional Scalar Functions;['multi-resolution', 'simplification', 'feature detection'];The Morse-Smale complex is an efficient representation of the gradient behavior of a scalar function, and critical points paired by the complex identify topological features and their importance. We present an algorithm that constructs the Morse-Smale complex in a series of sweeps through the data, identifying various components of the complex in a consistent manner. All components of the complex, both geometric and topological, are computed, providing a complete decomposition of the domain. Efficiency is maintained by representing the geometry of the complex in terms of point sets.;2007;http://dx.doi.org/10.1109/TVCG.2007.70552;efficient computation morsesmale complexes threedimensional scalar functions;morsesmale complex efficient representation gradient behavior scalar function critical points paired complex identify topological features importance algorithm constructs morsesmale complex series sweeps identifying various components complex consistent manner components complex geometric topological computed providing complete decomposition domain efficiency maintained representing geometry complex terms point sets
Topological Landscapes: A Terrain Metaphor for Scientific Data;['feature detection'];"Scientific visualization and illustration tools are designed to help people understand the structure and complexity of scientific data with images that are as informative and intuitive as possible. In this context the use of metaphors plays an important role since they make complex information easily accessible by using commonly known concepts. In this paper we propose a new metaphor, called \""topological landscapes,\"" which facilitates understanding the topological structure of scalar functions. The basic idea is to construct a terrain with the same topology as a given dataset and to display the terrain as an easily understood representation of the actual input data. In this projection from an n-dimensional scalar function to a two-dimensional (2D) model we preserve function values of critical points, the persistence (function span) of topological features, and one possible additional metric property (in our examples volume). By displaying this topologically equivalent landscape together with the original data we harness the natural human proficiency in understanding terrain topography and make complex topological information easily accessible.";2007;http://dx.doi.org/10.1109/TVCG.2007.70601;topological landscapes terrain metaphor scientific;scientific illustration tools designed help people structure complexity scientific images informative intuitive possible context use metaphors plays important role since make complex information easily accessible commonly known concepts propose metaphor called topological landscapes facilitates understanding topological structure scalar functions basic idea construct terrain topology dataset display terrain easily understood representation actual input projection ndimensional scalar function twodimensional model preserve function values critical points persistence function span topological features one possible additional metric property examples volume displaying topologically equivalent landscape together original harness natural human proficiency understanding terrain topography make complex topological information easily accessible
Multifield visualization using local statistical complexity;['feature detection'];Modern unsteady (multi-)field visualizations require an effective reduction of the data to be displayed. From a huge amount of information the most informative parts have to be extracted. Instead of the fuzzy application dependent notion of feature, a new approach based on information theoretic concepts is introduced in this paper to detect important regions. This is accomplished by extending the concept of local statistical complexity from finite state cellular automata to discretized (multi-)fields. Thus, informative parts of the data can be highlighted in an application-independent, purely mathematical sense. The new measure can be applied to unsteady multifields on regular grids in any application domain. The ability to detect and visualize important parts is demonstrated using diffusion, flow, and weather simulations.;2007;http://dx.doi.org/10.1109/TVCG.2007.70615;multifield local statistical complexity;modern unsteady multifield visualizations require effective reduction displayed huge amount information informative parts extracted instead fuzzy application dependent notion feature based information theoretic concepts introduced detect important regions accomplished extending concept local statistical complexity finite state cellular automata discretized multifields thus informative parts highlighted applicationindependent purely mathematical sense measure applied unsteady multifields regular grids application domain ability detect visualize important parts demonstrated diffusion flow weather simulations
Moment Invariants for the Analysis of 2D Flow Fields;['flowvisualization', 'feature detection'];We present a novel approach for analyzing two-dimensional (2D) flow field data based on the idea of invariant moments. Moment invariants have traditionally been used in computer vision applications, and we have adapted them for the purpose of interactive exploration of flow field data. The new class of moment invariants we have developed allows us to extract and visualize 2D flow patterns, invariant under translation, scaling, and rotation. With our approach one can study arbitrary flow patterns by searching a given 2D flow data set for any type of pattern as specified by a user. Further, our approach supports the computation of moments at multiple scales, facilitating fast pattern extraction and recognition. This can be done for critical point classification, but also for patterns with greater complexity. This multi-scale moment representation is also valuable for the comparative visualization of flow field data. The specific novel contributions of the work presented are the mathematical derivation of the new class of moment invariants, their analysis regarding critical point features, the efficient computation of a novel feature space representation, and based upon this the development of a fast pattern recognition algorithm for complex flow structures.;2007;http://dx.doi.org/10.1109/TVCG.2007.70579;moment invariants analysis flow fields;twodimensional flow field based idea invariant moments moment invariants traditionally computer vision applications adapted purpose interactive exploration flow field class moment invariants allows extract visualize flow patterns invariant translation scaling rotation one arbitrary flow patterns searching flow set type pattern specified user supports computation moments multiple scales facilitating fast pattern extraction recognition done critical point classification patterns greater complexity multiscale moment representation valuable comparative flow field specific contributions presented mathematical derivation class moment invariants analysis regarding critical point features efficient computation feature space representation based upon development fast pattern recognition algorithm complex flow structures
